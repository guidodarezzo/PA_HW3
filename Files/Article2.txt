While Hadoop remains one of the most popular platforms for big data, there's no single correct way to implement it in a manner best suited to your company's business requirementsWe delve into where IoT will have the biggest impact and what it means for the future of big data analytics. That's what MarketShare found out the hard way. The company, which is owned by Neustar, offers marketers analytics and reports that help them boost sales and make better decisions. Its clients include MasterCard, Turner Broadcasting and Twitter. MarketShare has been in business since 2005, and by 2012 found itself in a much different place, as Constellation Research VP and principal analyst Doug Henschen writes in a newly published case study. What started as a Small Data analysis challenge in the company's early days evolved into a Big Data challenge by 2012. That's when the firm shifted from analyzing gigabyte-scale data to using terabyte-scale data stored on Hadoop. The goal of using more data and a greater variety of data was to improve accuracy and to better measure digital activity across emerging mobile and social channels. Initially, MarketShare used Amazon's Elastic MapReduce service, which is powered by Hadoop. It then took data extracts from Hadoop jobs, moved them into an Oracle database, and finally generated reports using Tableau. This solution proved to be too slow and complicated, and had a key weakness: Users weren't able to drill down into the broader trend data for richer details. In early 2014, MarketShare began using Altiscale, a cloud-based Hadoop service that dramatically reduced processing times, but was still having challenges, as Henschen writes: With the embrace of digital campaign information, MarketShare needed to analyze data on a whole new scale. "Instead of looking at aggregated data on a weekly basis, we decided to look at spend related to every single individual ad impression," says Satya Ramachandran, MarketShare's head of engineering. "That entails looking at tens of terabytes instead of hundreds of gigabytes. Enterprise-Ready Hadoop. Today, organizations manage multi-layered data environments involving a wide array of data sources, data management processes, and data marts and warehouses. It is a challenge for enterprise IT teams to keep up with existing systems, let alone introducing new ones. Introducing Hadoop into the enterprise data architecture without interrupting the flow of established business processes, data access, and user data flows requires some careful thought once Hadoop gets beyond the experimental stage. Ensure that your platform provider has an architecture that allows for Big Data to be incorporated easily into the current ecosystem, so that companies maintain existing investments in environment, processes and people, and also have a method by which Big Data insights can be spread broadly throughout the organization. For many enterprises, a cloud implementation of Hadoop might be the fastest and easiest way to get to enterprise-ready Hadoop. For others, who prefer the direct control of on-premises, have adequate manpower to run it effectively in-house, and have the time for a full onsite deployment schedule, an on-premises provider with a strong professional services arm could be best. The Right Set of Tools for Your Jobs. Hadoop is a full ecosystem of solutions, not a single thing. Many processing engines run on top of Hadoop to fulfill a variety of different analytical requirements. Iterative analytics for machine learning, for example, are best achieved with fast-turn engines like Spark, while MapReduce, although older, is still the leader for massive batch processing jobs. Hive is a good choice for the middle ground. By choosing the enterprise-ready Hadoop, outsourcing operations, getting the right set of tools, and making peace with dirty data, companies can begin to extract business value from their Hadoop experience.

